{"name":"Phantom","tagline":"Asynchronous type-safe Scala DSL for Cassandra","body":"phantom [![Build Status](https://travis-ci.org/websudos/phantom.svg?branch=develop)](https://travis-ci.org/websudos/phantom) [![Coverage Status](https://coveralls.io/repos/websudos/phantom/badge.svg)](https://coveralls.io/r/websudos/phantom) [![Maven Central](https://maven-badges.herokuapp.com/maven-central/com.websudos/phantom_2.10/badge.svg)](https://maven-badges.herokuapp.com/maven-central/com.websudos/phantom_2.10)\r\n\r\n==============\r\nReactive type-safe Scala DSL for Cassandra\r\n\r\nTo stay up-to-date with our latest releases and news, follow us on Twitter: [@websudos](https://twitter.com/websudos).\r\n\r\nIf you use phantom, please consider adding your company to our list of adopters. Phantom is and will always be completely free and open source, \r\nbut the more adopters our projects have, the more people from our company will actively work to make them better.\r\n\r\n\r\n![phantom](https://s3-eu-west-1.amazonaws.com/websudos/oss/logos/phantom.png \"Websudos Phantom\")\r\n\r\n\r\nUsing phantom\r\n=============\r\n\r\n### Scala 2.10 and 2.11 releases ###\r\n\r\nWe publish phantom in 2 formats, stable releases and bleeding edge.\r\n\r\n- The stable release is always available on Maven Central and will have a version with a patch number \"0\". E.g \"x.x.0\".\r\n\r\n- Intermediary releases are available through our managed Maven repository,```\"Websudos releases\" at \"http://maven.websudos.co.uk/ext-release-local\"```.\r\n\r\n\r\n### Latest versions\r\n\r\n- Latest stable version: 1.8.0 (Maven Central)\r\n- Bleeding edge: 1.8.0 (Websudos Maven Repo)\r\n\r\nYou will also be needing the default resolvers for Maven Central and the typesafe releases. Phantom will never rely on any snapshots or be published as a\r\nsnapshot version, the bleeding edge is always subject to internal scrutiny before any releases into the wild.\r\n\r\nThe Apache Cassandra version used for auto-embedding Cassandra during tests is: ```val cassandraVersion = \"2.1.0-rc5\"```. You will require JDK 7 to use \r\nCassandra, otherwise you will get an error when phantom tries to start the embedded database. The recommended JDK is the Oracle variant.\r\n\r\n\r\n### Version highlights and upcoming features ###\r\n\r\n<ul>\r\n    <li><a href=\"#new-querybuilder\">1.8.0: A new QueryBuilder, written from the ground up, in idiomatic Scala</a></li>\r\n    <li><a href=\"#alter-queries\">1.8.0: Added support for type-safe ALTER queries</a></li>\r\n    <li><a href=\"#advanced-cql-support\">1.8.0: Support for advanced CQL options</a></li> \r\n    <li><a href=\"#prepared-statements\">1.9.0: Type safe prepared statements</a></li>\r\n    <li><a href=\"#automigration\">1.9.0: Automated Schema migrations</li>\r\n    <li><a href=\"#udts\">2.0.0: Type safe user defined types</li>\r\n    <li>\r\n      <a href=\"#breaking-changes\">Breaking changes in DSL and connectors\r\n      <ul>\r\n        <li><a href=\"#new-imports\">A new import structure</a></li>\r\n        <li><a href=\"#propagating-parse-errors\">Propagating parse errors</a></li>\r\n      </ul>\r\n    </a>\r\n    <li><a href=\"#autocreation\">1.9.0: Automated table creations</li>\r\n    <li><a href=\"#autotruncation\">1.9.0: Automated table truncation.</li>\r\n    <li><a href=\"#performance\">1.9.0: Big performance improvements</li>\r\n</ul>\r\n\r\n\r\n### Breaking API changes in Phantom 1.8.0 and beyond.\r\n\r\nThe 1.8.0 release constitutes a major re-working of a wide number of internal phantom primitives, including but not limited to a brand new Scala flavoured\r\nQueryBuilder with full support for all CQL 3 features and even some of the more \"esoteric\" options available in CQL. We went above and beyond to try and\r\noffer a tool that's comprehensive and doesn't miss out on any feature of the protocol, no matter how small.\r\n\r\nIf you are wondering what happened to 1.7.0, it was never publicly released as testing the new querybuilder entailed serious internal efforts and for such a drastic change\r\nwe wanted to do as much as possible to eliminate books. Surely there will be some still found, but hopefully very few and with your help they will be very short lived.\r\n\r\nDitching the Java Driver was not a question of code quality in the driver, but rather an opportunity to exploit the more advanced Scala type system features\r\nto introduce behaviour such as preventing duplicate limits on queries using phantom types, to prevent even more invalid queries from compiling, and to switch\r\n to a fully immutable QueryBuilder that's more in tone with idiomatic Scala, as opposed to the Java-esque mutable alternative already existing the java driver.\r\n\r\n\r\n<a id=\"new-imports\">A new import structure</a>\r\n================================================\r\n\r\n```import com.websudos.phantom.Implicits._``` has now been renamed to ```import com.websudos.phantom.dsl._```. The old import is still there but deprecated.\r\n\r\nA natural question you may ask is why we resorted to seemingly unimportant changes, but the goal here was to enforce the new implicit mechanism and use a uniform importing experience across all modules.\r\nSo you can have the series of ```import com.websudos.phantom.dsl._, import com.websudos.phantom.thrift._, import com.websudos.phantom.testkit._``` and so on, all identical, all using Scala ```package object``` definitions as intended.\r\n\r\n<a id=\"propagating-parse-errors\">Propagating parse errors</a>\r\n=============================================================\r\n\r\nUntil now, our implementation of Cassandra primitives has been based on the Datastax Java Driver and on an ```Option``` based DSL. This made it heard to deal with parse errors at runtime, specifically those situations when\r\nthe DSL was unable to parse the required type from the Cassandra result or in a simple case where ```null`` was returned for a non-optional column.\r\n\r\nThe core of the ```Column[Table, Record, ValueType].apply(value: ValueType]``` method which was used to parse rows in a type safe manner was written like this:\r\n\r\n```scala\r\n\r\nimport com.datastax.driver.core.Row\r\n\r\ndef apply(row: Row):  = optional(row).getOrElse(throw new Exception(\"Couldn't parse things\")\r\n\r\n```\r\n\r\nThis approach left the original exception which caused the parser to parse a ```null``` and subsequently a ```None``` was ignored.\r\n\r\nWith the new type-safe primitive interface that no longer relies on the Datastax Java driver we were also able to move the ```Option``` based parsing mechanism to a ```Try``` mechanism which will now\r\n log all parse errors un-altered, in the exact same way the are thrown at compile time, using the ```logger``` for the given table.\r\n \r\nInternally, we are now using something like this: \r\n \r\n```scala\r\n\r\n   def optional(r: Row): Try[T]\r\n \r\n   def apply(r: Row): T = optional(r) match {\r\n     case Success(value) => value\r\n     case Failure(ex) => {\r\n       table.logger.error(ex.getMessage)\r\n       throw ex\r\n     }\r\n   }\r\n\r\n```\r\n\r\nThe exception is now logged and propagated with no interference. We intercept it to provide consistent logging in the same table logger where you would naturally monitor for logs. \r\n\r\n\r\n<a id=\"improving-query-performance\">Improving query performance</a>\r\n==================================================================\r\n\r\nPlay enumerators and Twitter ResultSpools have been removed from the default ```one```, ```get```, ```fetch``` and ```collect``` methods. You will have to\r\nexplicitly call ```fetchEnumerator``` and ```fetchSpool``` if you want result throttling through async lazy iterators. This will offer everyone a signifact\r\nperformance improvement over query performance. Async iterators needed a lot of expensive \"magic\" to work properly, but you don't always need to fold over\r\n100k records. That behaviour was implemented both as means of showing off as well as doing all in one loads like the Spark - Cassandra connector performs. E.g\r\n dumping C* data into HDFS or whatever backup system. A big 60 - 70% gain should be expected.\r\n\r\nPhantom connectors now require an ```implicit com.websudos.phantom.connectors.KeySpace``` to be defined. Instead of using a plain string, you just have to\r\nuse ```KeySpace.apply``` or simply: ```trait MyConnector extends Connector { implicit val keySpace = KeySpace(\"your_def\") } ```. This change allows us to\r\nreplace the existing connector model and vastly improve the number of concurrent cluster connections required to perform operations on various keyspaces.\r\nInsteaed of the 1 per keyspace model, we can now successfully re-use the same session without evening needing to switch as phantom will use the full CQL\r\nreference syntax, e.g ```SELECT FROM keyspace.table``` instead of ```SELECY FROM table```.\r\n\r\nA entirely new set of options have been enabled in the type safe DSLs. You can now alter tables, specify advanced compressor behaviour and so forth, all\r\nfrom within phantom and with the guarantee of auto-completion and type safety.\r\n\r\n\r\n#### Support for ALTER queries.\r\n\r\nThis was never possible before in phantom, and now from 1.7.0 onwards we feature full support for using ALTER queries.\r\n\r\n\r\n\r\n<a id=\"table-of-contents\">Table of contents</a>\r\n===============================================\r\n\r\n<ul>\r\n  <li><a href=\"#issues-and-questions\">Issues and questions</a></li>\r\n  <li><a href=\"#adopters\">Adopters</a></li>\r\n  <li><a href=\"#roadmap\">Roadmap</a></li>\r\n  <li><a href=\"#learning-phantom\">Tutorials on phantom and Cassandra</a></li>\r\n  <li><a href=\"#commercial-support\">Commercial support</a></li>\r\n  <li><a href=\"#integrating-phantom-in-your-project\">Using phantom in your project</a></li>\r\n\r\n<li>\r\n  <p>Phantom columns</p>\r\n  <ul>\r\n    <li><a href=\"#primitive-columns\">Primitive columns</a></li>\r\n    <li><a href=\"#optional-primitive-columns\">Optional primitive columns</a></li>\r\n    <li><a href=\"#collection-columns\">Collection columns</a></li>\r\n    <li><a href=\"#collections-and-operators\">Collection operators</a></li>\r\n    <li><a href=\"#list-operators\">List operators</a></li>\r\n    <li><a href=\"#set-operators\">Set operators</a></li>\r\n    <li><a href=\"#map-operators\">Map operators</a></li>\r\n    <li><a href=\"#automated-schema-generation\">Automated schema generation</a></li>\r\n    <li>\r\n      <p><a href=\"#indexing-columns\">Indexing columns</a></p>\r\n      <ul>\r\n        <li><a href=\"#partition-key\">PartitionKey</a></li>\r\n        <li><a href=\"#primary-key\">PrimaryKey</a></li>\r\n        <li><a href=\"#secondary-index\">SecondaryIndex</a></li>\r\n        <li><a href=\"#clustering-order\">ClusteringOrder</a></li>\r\n      </ul>\r\n    </li>\r\n    <li><a href=\"#thrift-columns\">Thrift columns</a></li>\r\n  </ul>\r\n</li>\r\n<li><a href=\"#data-modeling-with-phantom\">Data modeling with phantom</a></li>\r\n<li>\r\n  <p><a href=\"#querying-with-phantom\">Querying with phantom</a></p>\r\n  <ul>\r\n    <li><a href=\"#common-query-methods\">Common query methods</a></li>\r\n    <li><a href=\"#select-queries\">SELECT queries</a></li>\r\n    <li><a href=\"#partial-select-queries\">Partial SELECT queries</a></li>\r\n    <li><a href=\"#where-and-operators\">WHERE and AND clause operators</a></li>\r\n    <li><a href=\"#create-queries\">CREATE queries</a></li>\r\n    <li><a href=\"#insert-queries\">INSERT queries</a></li>\r\n    <li><a href=\"#update-queries\">UPDATE queries</a></li>\r\n    <li><a href=\"#delete-queries\">DELETE queries</a></li>\r\n    <li><a href=\"#alter-queries\">ALTER queries</a></li>\r\n    <li><a href=\"#truncate-queries\">TRUNCATE queries</a></li>\r\n  </ul>\r\n  \r\n  <ul>\r\n    <p>Basic query examples</p>\r\n    <li><a href=\"#query-api\">Query API</a></li>\r\n    <li><a href=\"#scala-futures\">Using Scala Futures to query</a></li>\r\n    <li><a href=\"#scala-futures-examples\">Examples with Scala Futures</a></li>\r\n    <li><a href=\"#twitter-futures\">Using Twitter Futures to query</a></li>\r\n    <li><a href=\"#twitter-futures-examples\">Examples with Twitter Futures</a></li>\r\n  </ul>\r\n  \r\n<li>\r\n\r\n  <ul>\r\n    <p>Cassandra indexing</p>\r\n    <li><a href=\"#partition-tokens\">Using partition tokens</a></li>\r\n    <li><a href=\"#partition-token-operators\">Partition token operators</a></li>\r\n    <li><a href=\"#compound-keys\">Compound Keys</a></li>\r\n    <li><a href=\"#composite-keys\">Composite Keys</a></li>\r\n    <li><a href=\"#time-series\">Cassandra Time Series and ClusteringOrder</a></li>\r\n    <li><a href=\"#secondary-keys\">Secondary Keys</a></li>\r\n  </ul>\r\n\r\n<li><a href=\"#async-iterators\">Asynchronous iterators</a></li>\r\n<li>\r\n  <p>Batch statements</p>\r\n  <ul>\r\n    <li><a href=\"#logged-batch-statements\">LOGGED Batch statements</a></li>\r\n    <li><a href=\"#counter-batch-statements\">COUNTER Batch statements</a></li>\r\n    <li><a href=\"logged-batch-statements\">UNLOGGED Batch statements</a></li>\r\n  </ul>\r\n</li>\r\n\r\n\r\n<li><a href=\"#thrift-integration\">Thrift integration</a></li>\r\n\r\n<li>\r\n  <p><a href=\"apache-zookeeper-integration\">Apache ZooKeeper integration</a></p>\r\n  <ul>\r\n    <li><a href=\"#zookeeper-connectors\">ZooKeeper connectors</a></li>\r\n    <li><a href=\"#the-simple-cassandra-connector\">The simple Cassandra connector</a></li>\r\n    <li><a href=\"#the-default-zookeeper-connector-and-default-zookeeper-manager\">The DefaultZooKeeperConnector and DefaultZooKeeperManager</a></li>\r\n    <li><a href=\"#using-a-zookeeper-instance\">Using a ```com.websudos.util.zookeeper.ZooKeeperInstance```</a></li>\r\n  </ul>\r\n</li>\r\n\r\n<li>\r\n  <p><a href=\"#testing-utilities\">The phantom testkit</a></p>\r\n  <ul>\r\n    <li><a href=\"#auto-embedded-cassandra\">Auto-embeddeded Cassandra</a></li>\r\n    <li><a href=\"#using-the-default-suite\">Using the default PhantomCassandraSuite to write tests</a></li>\r\n    <li><a href=\"#using-the-automated-tools\">Using the phantom manager to auto-create, auto-migrate and auto-truncate tables</a></li>\r\n    <li>\r\n      <p><a href=\"#running-tests\">Running the tests locally</a></p>\r\n      <ul>\r\n        <li><a href=\"#scalatest-support\">ScalaTest support</a></li>\r\n        <li><a href=\"#specs2-support\">Specs2 support</a></li>\r\n      </ul>\r\n  </ul>\r\n</li>\r\n\r\n<li><a href=\"#contributors\">Contributing to phantom</a></li>\r\n<li><a href=\"#using-gitflow\">Using GitFlow as a branching model</a></li>\r\n<li><a href=\"#scala-style-guidelines\">Scala style guidelines for contributions</a></li>\r\n<li><a href=\"#copyright\">Copyright</a></li>\r\n\r\n\r\n<a id=\"issues-and-questions\">Issues and questions</a>\r\n=====================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nWe love Cassandra to bits and use it in every bit our stack. phantom makes it super trivial for Scala users to embrace Cassandra.\r\n\r\nCassandra is highly scalable and it's by far the most powerful database technology available, open source or otherwise.\r\n\r\nPhantom is built on top of the [Datastax Java Driver](https://github.com/datastax/java-driver), which does most of the heavy lifting. \r\n\r\nIf you're completely new to Cassandra, a much better place to start is the [Datastax Introduction to Cassandra](http://www.datastax.com/documentation/getting_started/doc/getting_started/gettingStartedIntro_r.html). An even better introduction is available on [our blog]\r\n(http://blog.websudos.com/category/nosql/cassandra/), where we have a full series of introductory posts to Cassandra with phantom.\r\n\r\nWe are very happy to help implement missing features in phantom, answer questions about phantom, and occasionally help you out with Cassandra questions! Please use GitHub for any issues or bug reports.\r\n\r\nAdopters\r\n========\r\n\r\nThis is a list of companies that have embraced phantom as part of their technology stack and using it in production environments.\r\n\r\n- [CreditSuisse](https://www.credit-suisse.com/global/en/)\r\n- [Pellucid Analytics](http://www.pellucid.com/)\r\n- [Sphonic](http://www.sphonic.com/)\r\n- [websudos](https://www.websudos.com/)\r\n- [Equens](http://www.equens.com/)\r\n- [VictorOps](http://www.victorops.com/)\r\n- [Socrata](http://www.socrata.com)\r\n\r\nRoadmap\r\n========\r\n\r\nWhile dates are not fixed, we will use this list to tell you about our plans for the future. If you have great ideas about what could benefit all phantom \r\nadopters, please get in touch. We are very happy and eager to listen.\r\n\r\n- User defined types\r\n\r\nWe are working closely around the latest features in the Datastax Java driver and Apache Cassandra 2.1 to offer a fully type safe DSL for user defined types.\r\nThis feature is well in progress and you can expect to see it live roughly at the same time as the release of the Datastax 2.1 driver, planned for July 2014.\r\n\r\nSome of the cool features include automatic schema generation, fully type safe referencing of fields and inner members of UDTs and fully type safe querying.\r\n\r\n\r\n- Spark integration\r\n\r\nThanks to the recent partnership between Databricks and Datastax, Spark is getting a Cassandra facelift with a Datastax backed integration. We won't be slow to\r\nfollow up with a type safe Scala variant of that integration, so you can enjoy the benefits of high power computation with Cassandra as a backup\r\n storage through the simple high power DSL we've gotten you used to.\r\n\r\n- Prepared statements\r\n\r\nBy popular demand, a feature long overdue in phantom. The main reason is the underlying Java driver and the increased difficulty of guaranteeing type safety\r\nwith prepared statements along with a nice DSL to get things done. Not to say it's impossible, this will be released after the new query builder emerges.\r\n\r\n- A new QueryBuilder(available as of 1.6.0)\r\n\r\n- Zookeeper support(available as of 1.1.0).\r\n\r\n\r\n<a id=\"learning-phantom\">Tutorials on phantom and Cassandra</a>\r\n======================================================================\r\n\r\nCommercial support\r\n===================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nWe, the people behind phantom run a software development house specialised in Scala and NoSQL. If you are after enterprise grade\r\ntraining or support for using phantom, [Websudos](http://websudos.com) is here to help!\r\n\r\nWe offer a comprehensive range of elite Scala development services, including but not limited to:\r\n\r\n- Software development\r\n- Remote contractors for hire\r\n- Advanced Scala and Cassandra training\r\n\r\n\r\nWe are big fans of open source and we will open source every project we can! To read more about our OSS efforts, \r\nclick [here](http://www.websudos.co.uk/work).\r\n\r\n\r\n<a id=\"integrating-phantom\">Integrating phantom in your project</a>\r\n===================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThe resolvers needed for Phantom are the Typesafe defaults, Sonatype, Twitter and our very own. The below list should make sure you have no dependency\r\nresolution errors.\r\n\r\n```scala\r\nresolvers ++= Seq(\r\n  \"Typesafe repository snapshots\" at \"http://repo.typesafe.com/typesafe/snapshots/\",\r\n  \"Typesafe repository releases\" at \"http://repo.typesafe.com/typesafe/releases/\",\r\n  \"Sonatype repo\"                    at \"https://oss.sonatype.org/content/groups/scala-tools/\",\r\n  \"Sonatype releases\"                at \"https://oss.sonatype.org/content/repositories/releases\",\r\n  \"Sonatype snapshots\"               at \"https://oss.sonatype.org/content/repositories/snapshots\",\r\n  \"Sonatype staging\"                 at \"http://oss.sonatype.org/content/repositories/staging\",\r\n  \"Java.net Maven2 Repository\"       at \"http://download.java.net/maven/2/\",\r\n  \"Twitter Repository\"               at \"http://maven.twttr.com\",\r\n  \"Websudos releases\"                at \"http://maven.websudos.co.uk/ext-release-local\"\r\n)\r\n```\r\n\r\nFor most things, all you need is ```phantom-dsl``` and ```phantom-testkit```. Read through for information on other modules.\r\n\r\n```scala\r\nlibraryDependencies ++= Seq(\r\n  \"com.websudos\"  %% \"phantom-dsl\"                   % phantomVersion,\r\n  \"com.websudos\"  %% \"phantom-testkit\"               % phantomVersion\r\n)\r\n```\r\n\r\nThe full list of available modules is:\r\n\r\n```scala\r\nlibraryDependencies ++= Seq(\r\n  \"com.websudos\"  %% \"phantom-dsl\"                   % phantomVersion,\r\n  \"com.websudos\"  %% \"phantom-example\"               % phantomVersion,\r\n  \"com.websudos\"  %% \"phantom-scalatra\"              % phantomVersion,\r\n  \"com.websudos\"  %% \"phantom-spark\"                 % phantomVersion,\r\n  \"com.websudos\"  %% \"phantom-thrift\"                % phantomVersion,\r\n  \"com.websudos\"  %% \"phantom-testkit\"               % phantomVersion,\r\n  \"com.websudos\"  %% \"phantom-udt\"                   % phantomVersion,\r\n  \"com.websudos\"  %% \"phantom-zookeeper\"             % phantomVersion\r\n)\r\n```\r\nIf you include ```phantom-zookeeper```, make sure to add the following resolvers:\r\n\r\n```scala\r\nresolvers += \"twitter-repo\" at \"http://maven.twttr.com\"\r\n\r\nresolvers += \"websudos-repo\" at \"http://maven.websudos.co.uk/ext-release-local\"\r\n```\r\n\r\n<a id=\"primitive-columns\">Primitive columns</a>\r\n====================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThis is the list of available columns and how they map to C* data types.\r\nThis also includes the newly introduced ```static``` columns in C* 2.0.6.\r\n\r\nThe type of a static column can be any of the allowed primitive Cassandra types.\r\nphantom won't let you mixin a non-primitive via implicit magic.\r\n\r\n| phantom columns               | Java/Scala type           | Cassandra type    |\r\n| ---------------               |-------------------        | ----------------- |\r\n| BlobColumn                    | java.nio.ByteBuffer       | blog              |\r\n| BigDecimalColumn              | scala.math.BigDecimal     | decimal           |\r\n| BigIntColumn                  | scala.math.BigInt         | varint            |\r\n| BooleanColumn                 | scala.Boolean             | boolean           |\r\n| DateColumn                    | java.util.Date            | timestamp         |\r\n| DateTimeColumn                | org.joda.time.DateTime    | timestamp         |\r\n| DoubleColumn                  | scala.Double              | double            |\r\n| EnumColumn                    | scala.Enumeration         | text              |\r\n| FloatColumn                   | scala.Float               | float             |\r\n| IntColumn                     | scala.Int                 | int               |\r\n| InetAddressColumn             | java.net.InetAddress      | inet              |\r\n| LongColumn                    | scala.Long                | long              |\r\n| StringColumn                  | java.lang.String          | text              |\r\n| UUIDColumn                    | java.util.UUID            | uuid              |\r\n| TimeUUIDColumn                | java.util.UUID            | timeuuid          |\r\n| CounterColumn                 | scala.Long                | counter           |\r\n| StaticColumn&lt;type&gt;      | &lt;type&gt;              | type static       |\r\n\r\n\r\n<a id=\"optional-primitive-columns\">Optional primitive columns</a>\r\n===================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nOptional columns allow you to set a column to a ```null``` or a ```None```. Use them when you really want something to be optional.\r\nThe outcome is that instead of a ```T``` you get an ```Option[T]``` and you can ```match, fold, flatMap, map``` on a ```None```.\r\n\r\nThe ```Optional``` part is handled at a DSL level, it's not translated to Cassandra in any way.\r\n\r\n| phantom columns               | Java/Scala type                   | Cassandra columns |\r\n| ---------------               | -------------------------         | ----------------- |\r\n| OptionalBlobColumn            | Option[java.nio.ByteBuffer]       | blog              |\r\n| OptionalBigDecimalColumn      | Option[scala.math.BigDecimal]     | decimal           |\r\n| OptionalBigIntColumn          | Option[scala.math.BigInt]         | varint            |\r\n| OptionalBooleanColumn         | Option[scala.Boolean]             | boolean           |\r\n| OptionalDateColumn            | Option[java.util.Date]            | timestamp         |\r\n| OptionalDateTimeColumn        | Option[org.joda.time.DateTime]    | timestamp         |\r\n| OptionalDoubleColumn          | Option[scala.Double]              | double            |\r\n| OptionalEnumColumn            | Option[scala.Enumeration]         | text              |\r\n| OptionalFloatColumn           | Option[scala.Float]               | float             |\r\n| OptionalIntColumn             | Option[scala.Int]                 | int               |\r\n| OptionalInetAddressColumn     | Option[java.net.InetAddress]      | inet              |\r\n| OptionalLongColumn            | Option[Long]                      | long              |\r\n| OptionalStringColumn          | Option[java.lang.String]          | text              |\r\n| OptionalUUIDColumn            | Option[java.util.UUID]            | uuid              |\r\n| OptionalTimeUUID              | Option[java.util.UUID]            | timeuuid          |\r\n\r\n\r\n<a id=\"collection-columns\">Collection columns</a>\r\n======================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nCassandra collections do not allow custom data types. Storing JSON as a string is possible, but it's still a ```text``` column as far as Cassandra is concerned.\r\nThe ```type``` in the below example is always a default C* type.\r\n\r\nJSON columns require you to define a ```toJson``` and ```fromJson``` method, telling phantom how to go from a ```String``` to the type you need. \r\nIt makes no assumptions as to what library you are using, although we have tested with ```lift-json``` and ```play-json```.\r\n\r\nExamples on how to use JSON columns can be found in [JsonColumnTest.scala](https://github.com/websudos/phantom/blob/develop/phantom-dsl/src/test/scala/com/websudos/phantom/dsl/specialized/JsonColumnTest.scala)\r\n\r\n| phantom columns                     | Cassandra columns       |\r\n| ---------------                     | -----------------       |\r\n| ListColumn.&lt;type&gt;             | list&lt;type&gt;        |\r\n| SetColumn.&lt;type&gt;              | set&lt;type&gt;         |\r\n| MapColumn.&lt;type, type&gt;        | map&lt;type, type&gt;   |\r\n| JsonColumn.&lt;type&gt;             | text                    |\r\n| JsonListColumn.&lt;type&gt;         | list&lt;text&gt;        |\r\n| JsonSetColumn.&lt;type&gt;          | set&lt;type&gt;         |\r\n\r\n<a id=\"indexing-columns\">Indexing columns</a>\r\n==========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nphantom uses a specific set of traits to enforce more advanced Cassandra limitations and schema rules at compile time.\r\nInstead of waiting for Cassandra to tell you you've done bad things, phantom won't let you compile them, saving you a lot of time.\r\n\r\nThe error messages you get when your model is off with respect to Cassandra rules is not particularly helpful and we are working on a better builder to allow\r\n for better error messages. Until then, if you see things like:\r\n\r\n```scala\r\n\r\nimport com.websudos.phantom.dsl._\r\n\r\ncase class Student(id: UUID, name: String)\r\n\r\nclass Students extends CassandraTable[Students, Student] {\r\n  object id extends UUIDColumn(this) with PartitionKey[UUID]\r\n  object name extends StringColumn(this)\r\n\r\n  def fromRow(row: Row): Student = {\r\n    Student(id(row), name(row))\r\n  }\r\n}\r\n\r\nobject Students extends Students with Connector {\r\n\r\n  /**\r\n   * The below code will result in a compilation error phantom produces by design.\r\n   * This behaviour is not only correct with respect to CQL but also intended by the implementation.\r\n   *\r\n   * The reason why it won't compile is because the \"name\" column is not an index in the \"Students\" table, which means using \"name\" in a \"where\" clause is\r\n   * invalid CQL. Phantom prevents you from running most invalid queries by simply giving you a compile time error instead.\r\n   */\r\n  def getByName(name: String): Future[Option[Student]] = {\r\n    select.where(_.name eqs name).one()\r\n  }\r\n}\r\n```\r\n\r\nThe compilation error message for the above looks something like this:\r\n\r\n```scala\r\n value eqs is not a member of object x$9.name\r\n```\r\n\r\nMight seem overly mysterious to start with, but the logic is simple. There is no implicit conversion in scope to convert your non-indexed column to a ```QueryColumn```. If you don't have an index, you can't query.\r\n\r\n```scala\r\n  Students.update.where(_.id eqs someId).onlyIf(_.name is \"test\")\r\n```\r\n\r\n\r\n<a id=\"partition-key\">PartitionKey</a>\r\n==============================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThis is the default partitioning key of the table, telling Cassandra how to divide data into partitions and store them accordingly.\r\nYou must define at least one partition key for a table. Phantom will gently remind you of this with a fatal error.\r\n\r\nIf you use a single partition key, the ```PartitionKey``` will always be the first ```PrimaryKey``` in the schema.\r\n\r\nIt looks like this in CQL: ```PRIMARY_KEY(your_partition_key, primary_key_1, primary_key_2)```.\r\n\r\nUsing more than one ```PartitionKey[T]``` in your schema definition will output a Composite Key in Cassandra.\r\n```PRIMARY_KEY((your_partition_key_1, your_partition_key2), primary_key_1, primary_key_2)```.\r\n\r\n\r\n<a id=\"primary-key\">PrimaryKey</a>\r\n==============================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nAs it's name says, using this will mark a column as ```PrimaryKey```. Using multiple values will result in a Compound Value.\r\nThe first ```PrimaryKey``` is used to partition data. phantom will force you to always define a ```PartitionKey``` so you don't forget\r\nabout how your data is partitioned. We also use this DSL restriction because we hope to do more clever things with it in the future.\r\n\r\nA compound key in C* looks like this:\r\n```PRIMARY_KEY(primary_key, primary_key_1, primary_key_2)```.\r\n\r\nBefore you add too many of these, remember they all have to go into a ```where``` clause.\r\nYou can only query with a full primary key, even if it's compound. phantom can't yet give you a compile time error for this, but Cassandra will give you a runtime one.\r\n\r\n<a id=\"secondary-index\">SecondaryIndex</a>\r\n==============================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThis is a SecondaryIndex in Cassandra. It can help you enable querying really fast, but it's not exactly high performance.\r\nIt's generally best to avoid it, we implemented it to show off what good guys we are.\r\n\r\nWhen you mix in ```Index[T]``` on a column, phantom will let you use it in a ```where``` clause.\r\nHowever, don't forget to ```allowFiltering``` for such queries, otherwise C* will give you an error.\r\n\r\n<a id=\"clustering-order\">ClusteringOrder</a>\r\n=================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThis can be used with either ```java.util.Date``` or ```org.joda.time.DateTime```. It tells Cassandra to store records in a certain order based on this field.\r\n\r\nAn example might be: ```object timestamp extends DateTimeColumn(this) with ClusteringOrder[DateTime] with Ascending```\r\nTo fully define a clustering column, you MUST also mixin either ```Ascending``` or ```Descending``` to indicate the sorting order.\r\n\r\n\r\n\r\n<a id=\"thrift-columns\">Thrift Columns</a>\r\n==========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThese columns are especially useful if you are building Thrift services. They are deeply integrated with Twitter Scrooge and relevant to the Twitter ecosystem(Finagle, Zipkin, Storm etc)\r\nThey are available via the ```phantom-thrift``` module and you need to import the Thrift package to get all necessary types into scope.\r\n\r\n ```scala\r\n import com.websudos.phantom.thrift._\r\n ```\r\n\r\nIn the below scenario, the Cassandra type is always text and the type you need to pass to the column is a Thrift struct, specifically ```com.twitter.scrooge\r\n.ThriftStruct```.\r\nphantom will use a ```CompactThriftSerializer```, store the record as a binary string and then reparse it on fetch.\r\n\r\nThrift serialization and de-serialization is extremely fast, so you don't need to worry about speed or performance overhead.\r\nYou generally use these to store collections(small number of items), not big things.\r\n\r\n| phantom columns                     | Cassandra columns       |\r\n| ---------------                     | -----------------       |\r\n| ThriftColumn.&lt;type&gt;           | text                    |\r\n| ThriftListColumn.&lt;type&gt;       | list&lt;text&gt;        |\r\n| ThriftSetColumn.&lt;type&gt;        | set&lt;text&gt;         |\r\n| ThriftMapColumn.&lt;type, type&gt;  | map&lt;text, text&gt;   |\r\n\r\n\r\n\r\n<a id=\"data-modeling\">Data modeling with phantom</a>\r\n====================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n```scala\r\n\r\nimport java.util.Date\r\nimport com.websudos.phantom.sample.ExampleModel\r\nimport com.websudos.phantom.dsl._\r\n\r\ncase class ExampleModel (\r\n  id: Int,\r\n  name: String,\r\n  props: Map[String, String],\r\n  timestamp: Int,\r\n  test: Option[Int]\r\n)\r\n\r\nsealed class ExampleRecord extends CassandraTable[ExampleRecord, ExampleModel] {\r\n\r\n  object id extends UUIDColumn(this) with PartitionKey[UUID]\r\n  object timestamp extends DateTimeColumn(this) with ClusteringOrder[DateTime] with Ascending\r\n  object name extends StringColumn(this)\r\n  object props extends MapColumn[ExampleRecord, ExampleModel, String, String](this)\r\n  object test extends OptionalIntColumn(this)\r\n\r\n  def fromRow(row: Row): ExampleModel = {\r\n    ExampleModel(id(row), name(row), props(row), timestamp(row), test(row));\r\n  }\r\n}\r\n\r\n```\r\n\r\n<a id=\"querying-with-phantom\">Querying with Phantom</a>\r\n=======================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThe query syntax is inspired by the Foursquare Rogue library and aims to replicate CQL 3 as much as possible.\r\n\r\nPhantom works with both Scala Futures and Twitter Futures as first class citizens.\r\n\r\n\r\n<a id=\"common-query-methods\">Common query methods</a>\r\n=====================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThe full list can be found in [CQLQuery.scala](https://github.com/websudos/phantom/blob/develop/phantom-dsl/src/main/scala/com/websudos/phantom/query/CQLQuery\r\n.scala).\r\n\r\n| Method name                       | Description                                                                           |\r\n| --------------------------------- | ------------------------------------------------------------------------------------- |\r\n| ```tracing_=```                   | The Cassandra utility method. Enables or disables tracing.                            |\r\n| ```queryString```                 | Get the output CQL 3 query of a phantom query.                                        |\r\n| ```consistencyLevel```            | Retrieves the consistency level in use.                                               |\r\n| ```consistencyLevel_=```          | Sets the consistency level to use.                                                    |\r\n| ```retryPolicy```                 | Retrieves the RetryPolicy in use.                                                     |\r\n| ```retryPolicy_=```               | Sets the RetryPolicy to use.                                                          |\r\n| ```serialConsistencyLevel```      | Retrieves the serial consistency level in use.                                        |\r\n| ```serialConsistencyLevel_=```    | Sets the serial consistency level to use.                                             |\r\n| ```forceNoValues_=```             | Sets the serial consistency level to use.                                             |\r\n| ```routingKey```                  | Retrieves the Routing Key as a ByteBuffer.                                            |\r\n\r\n\r\n<a id=\"select-queries\">Select queries</a>\r\n================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n| Method name                       | Description                                                                           |\r\n| --------------------------------- | ------------------------------------------------------------------------------------- |\r\n| ```where```                       | The ```WHERE``` clause in CQL                                                         |\r\n| ```and```                         | Chains several clauses, creating a ```WHERE ... AND``` query                          |\r\n| ```orderBy```                     | Adds an ```ORDER_BY column_name``` to the query                                       |\r\n| ```allowFiltering```              | Allows Cassandra to filter records in memory. This is an expensive operation.         |\r\n| ```limit```                       | Sets the exact number of records to retrieve.                                         |\r\n\r\n\r\nSelect queries are very straightforward and enforce most limitations at compile time.\r\n\r\n\r\n<a id=\"where-and-operators\">```where``` and ```and``` clause operators</a>\r\n==========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n| Operator name      | Description                                                              |\r\n| ------------------ | ------------------------------------------------------------                                             |\r\n| eqs                | The \"equals\" operator. Will match if the objects are equal                                               |\r\n| in                 | The \"in\" operator. Will match if the object is found the list of arguments                               |\r\n| gt                 | The \"greater than\" operator. Will match a the record is greater than the argument and exists             |\r\n| gte                | The \"greater than or equals\" operator. Will match a the record is greater than the argument and exists   |\r\n| lt                 | The \"lower than\" operator. Will match a the record that is less than the argument and exists             |\r\n| lte                | The \"lower than or equals\" operator. Will match a the record that is less than the argument and exists   |\r\n\r\n\r\n<a id=\"partial-select-queries\">Partial selects</a>\r\n===================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nAll partial select queries will return Tuples and are therefore limited to 22 fields.\r\nWe haven't yet bothered to add more than 10 fields in the select, but you can always do a Pull Request.\r\nThe file you are looking for is [here](https://github.com/websudos/phantom/blob/develop/phantom-dsl/src/main/scala/com/websudos/phantom/SelectTable.scala).\r\nThe 22 field limitation will change in Scala 2.11 and phantom will be updated once cross version compilation is enabled.\r\n\r\n```scala\r\n  def getNameById(id: UUID): Future[Option[String]] = {\r\n    ExampleRecord.select(_.name).where(_.id eqs someId).one()\r\n  }\r\n\r\n  def getNameAndPropsById(id: UUID): Future[Option(String, Map[String, String])] {\r\n    ExampleRecord.select(_.name, _.props).where(_.id eqs someId).one()\r\n  }\r\n```\r\n\r\n<a id=\"insert-queries\">\"Insert\" queries</a>\r\n==========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n| Method name                       | Description                                                                           |\r\n| --------------------------------- | ------------------------------------------------------------------------------------- |\r\n| ```value```                       | A type safe Insert query builder. Throws an error for ```null``` values.              |\r\n| ```valueOrNull```                 | This will accept a ```null``` without throwing an error.                              |\r\n| ```ttl```                         | Sets the \"Time-To-Live\" for the record.                                               |\r\n\r\n\r\n<a id=\"update-queries\">\"Update\" queries</a>\r\n==========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n| Method name                       | Description                                                                           |\r\n| --------------------------------- | ------------------------------------------------------------------------------------- |\r\n| ```where```                       | The ```WHERE``` clause in CQL                                                         |\r\n| ```and```                         | Chains several clauses, creating a ```WHERE ... AND``` query                          |\r\n| ```modify```                      | The actual update query builder                                                       |\r\n| ```onlyIf```                     | Addition update condition. Used on non-primary columns                                |\r\n\r\nExample:\r\n\r\n```scala\r\nExampleRecord.update\r\n  .where(_.id eqs myUuid)\r\n  .modify(_.name setTo \"Barack Obama\")\r\n  .and(_.props put (\"title\" -> \"POTUS\"))\r\n  .future()\r\n```\r\n\r\n\r\n<a id=\"delete-queries\">\"Delete\" queries</a>\r\n===========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n| Method name                       | Description                                                                           |\r\n| --------------------------------- | ------------------------------------------------------------------------------------- |\r\n| ```where```                       | The ```WHERE``` clause in CQL                                                         |\r\n| ```and```                         | Chains several clauses, creating a ```WHERE ... AND``` query                          |\r\n\r\n\r\nDelete queries are very simple ways to either delete a row or alternatively set a column to ```null```. For instance:\r\n\r\n\r\n```scala\r\n\r\nBasicTable.update.where(_.id eqs someId).modify(_.someSet setTo Set.empty[String])\r\n\r\n// is actually equivalent to\r\n\r\nBasicTable.delete(_.someSet).where(_.id eqs someId)\r\n\r\n```\r\n\r\n\r\n\r\n<a id=\"query-api\">Query API</a>\r\n===============================\r\n\r\nPhantom offers a dual query API based on Scala concurrency primitives, which makes it trivial to use phantom in most known frameworks, such as Play!, Spray,\r\nAkka, Scruffy, Lift, and many others. Integration is trivial and easily achievable, all you have to do is to use the Scala API methods and you get out of the\r\n box integration.\r\n\r\nPhantom also offers another API based on Twitter proprietary concurrency primitives. This is due to the fact that internally we rely very heavily on the\r\nTwitter eco-system. It's why phantom also offers Finagle-Thrift support out of the box and integrates with Twitter Scrooge. It fits in perfectly with\r\napplications powered by Finagle RPC, Zipkin, Thrift, Ostrich, Aurora, Mesos, and the rest of the Twitter lot.\r\n\r\n\r\n| Method name                        | Description                                                                           | Scala result type |\r\n| ---------------------------------- | ------------------------------------------------------------------------------------- | ------------------|\r\n| ```future```                       | Executes a command and returns a ```ResultSet```. This is useful when you don't need to return a value.| ```scala.concurrent.Future[ResultSet]``` |\r\n| ```execute```                       | Executes a command and returns a ```ResultSet```. This is useful when you don't need to return a value.| ```com.twitter.util.Future[ResultSet]``` |\r\n| ```one```                          | Executes a command and returns an ```Option[T]```. Use this when you are selecting and you only need one value. Adds ```LIMIT 1``` to the CQL query. | ```scala.concurrent.Future[Option[Record]]``` |\r\n| ```get```                          | Executes a command and returns an ```Option[T]```. Use this when you are selecting and you only need one value. Adds```LIMIT 1``` to the CQL query. | ```com.twitter.util.Future[Option[Record]]``` |\r\n| ```fetch```                          | Returns a sequence of matches. Use when you expect more than 1 match. | ```scala.concurrent.Future[Seq[Record]]``` |.\r\n| ```collect```                          |  Returns a sequence of matches. Use when you expect more than 1 match. | ```com.twitter.util.Future[Seq[Record]``` |\r\n| ```fetchSpool```                        | This is useful when you need the underlying ResultSpool.                        | ```com.twitter.concurrent.Spool[T]]``` |\r\n| ```fetchEnumerator```                        | This is useful when you need the underlying Play based enumerator.                        | ```play.api.libs.iteratee.Enumerator[T]``` |\r\n\r\n\r\n\r\n\r\n\r\n<a id=\"scala-futures\">Scala Futures</a>\r\n=======================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n\r\nPhantom offers a dual asynchronous Future API for the completion of tasks, ```scala.concurrent.Future``` and ```com.twitter.util.Future```.\r\nHowever, the concurrency primitives are all based on Google Guava executors and listening decorators. The future API is just for the convenience of users.\r\nThe Scala Future methods are: \r\n\r\n\r\n```scala\r\nExampleRecord.select.one() // When you only want to select one record\r\nExampleRecord.update.where(_.name eqs name).modify(_.name setTo \"someOtherName\").future() // When you don't care about the return type.\r\nExampleRecord.select.fetchEnumerator // when you need an Enumerator\r\nExampleRecord.select.fetch // When you want to fetch a Seq[Record]\r\n```\r\n\r\n<a id=\"scala-futures-examples\">Examples with Scala Futures</a>\r\n================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n\r\n```scala\r\n\r\nimport scala.concurrent.ExecutionContext.Implicits.global\r\nimport scala.concurrent.Future\r\n\r\nobject ExampleRecord extends ExampleRecord {\r\n  override val tableName = \"examplerecord\"\r\n\r\n  // now define a session, a normal Datastax cluster connection\r\n  implicit val session = SomeCassandraClient.session;\r\n\r\n  def getRecordsByName(name: String): Future[Seq[ExampleModel]] = {\r\n    ExampleRecord.select.where(_.name eqs name).fetch\r\n  }\r\n\r\n  def getOneRecordByName(name: String, someId: UUID): Future[Option[ExampleModel]] = {\r\n    ExampleRecord.select.where(_.name eqs name).and(_.id eqs someId).one()\r\n  }\r\n}\r\n```\r\n\r\n<a id=\"twitter-futures\">Twitter Futures</a>\r\n===========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nPhantom doesn't depend on Finagle for this, we are simply using ```\"com.twitter\" %% \"util-core\" % Version\"``` to return a ```com.twitter.util.Future```. \r\nHowever, the concurrency primitives are all based on Google Guava executors and listening decorators. The future API is just for the convenience of users.\r\n\r\n\r\n```scala\r\nExampleRecord.select.get() // When you only want to select one record\r\nExampleRecord.update.where(_.name eqs name).modify(_.name setTo \"someOtherName\").execute() // When you don't care about the return type.\r\nExampleRecord.select.enumerate // when you need an Enumerator\r\nExampleRecord.select.collect // When you want to fetch a Seq[Record]\r\n```\r\n\r\n<a id=\"twitter-futures-examples\">More examples with Twitter Futures</a>\r\n=======================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n```scala\r\n\r\nimport com.twitter.util.Future\r\n\r\nobject ExampleRecord extends ExampleRecord {\r\n  override val tableName = \"examplerecord\"\r\n\r\n  // now define a session, a normal Datastax cluster connection\r\n  implicit val session = SomeCassandraClient.session;\r\n\r\n  def getRecordsByName(name: String): Future[Seq[ExampleModel]] = {\r\n    ExampleRecord.select.where(_.name eqs name).collect\r\n  }\r\n\r\n  def getOneRecordByName(name: String, someId: UUID): Future[Option[ExampleModel]] = {\r\n    ExampleRecord.select.where(_.name eqs name).and(_.id eqs someId).get()\r\n  }\r\n}\r\n```\r\n\r\n<a id=\"collections-and-operators\">Collections and operators</a>\r\n================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nBased on the above list of columns, phantom supports CQL 3 modify operations for CQL 3 collections: ```list, set, map```.\r\nAll operators will be available in an update query, specifically:\r\n\r\n```ExampleRecord.update.where(_.id eqs someId).modify(_.someList $OPERATOR $args).future()```.\r\n\r\n<a id=\"list-operators\">List operators</a>\r\n==========================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nExamples in [ListOperatorsTest.scala](https://github.com/websudos/phantom/blob/develop/phantom-dsl/src/test/scala/com/websudos/phantom/dsl/crud/ListOperatorsTest.scala).\r\n\r\n| Name                          | Description                                   |\r\n| ----------------------------- | --------------------------------------------- |\r\n| ```prepend```                 | Adds an item to the head of the list          |\r\n| ```prependAll```              | Adds multiple items to the head of the list   |\r\n| ```append```                  | Adds an item to the tail of the list          |\r\n| ```appendAll```               | Adds multiple items to the tail of the list   |\r\n| ```discard```                 | Removes the given item from the list.         |\r\n| ```discardAll```              | Removes all given items from the list.        |\r\n| ```setIdIx```                 | Updates a specific index in the list          |\r\n\r\n<a id=\"set-operators\">Set operators</a>\r\n=======================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nSets have a better performance than lists, as the Cassandra documentation suggests.\r\nExamples in [SetOperationsTest.scala](https://github.com/websudos/phantom/blob/develop/phantom-test/src/test/scala/com/websudos/phantom/dsl/crud/SetOperationsTest.scala).\r\n\r\n| Name                          | Description                                   |\r\n| ----------------------------- | --------------------------------------------- |\r\n| ```add```                     | Adds an item to the tail of the set           |\r\n| ```addAll```                  | Adds multiple items to the tail of the set    |\r\n| ```remove ```                 | Removes the given item from the set.          |\r\n| ```removeAll```               | Removes all given items from the set.         |\r\n\r\n\r\n<a id=\"map-operators\">Map operators</a>\r\n=======================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nBoth the key and value types of a Map must be Cassandra primitives.\r\nExamples in [MapOperationsTest.scala](https://github.com/websudos/phantom/blob/develop/phantom-test/src/test/scala/com/websudos/phantom/dsl/crud/MapOperationsTest.scala):\r\n\r\n| Name                          | Description                                   |\r\n| ----------------------------- | --------------------------------------------- |\r\n| ```put```                     | Adds an (key -> value) pair to the map        |\r\n| ```putAll```                  | Adds multiple (key -> value) pairs to the map |\r\n\r\n\r\n<a id=\"automated-schema-generation\">Automated schema generation</a>\r\n===================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nReplication strategies and more advanced features are not yet available in phantom, but CQL 3 Table schemas are  automatically generated from the Scala code. To create a schema in Cassandra from a table definition:\r\n\r\n```scala\r\n\r\nimport scala.concurrent.Await\r\nimport scala.concurrent.duration._\r\n\r\nAwait.result(ExampleRecord.create().future(), 5000 millis)\r\n```\r\n\r\nOf course, you don't have to block unless you want to.\r\n\r\n\r\n<a id=\"partition-tokens\">Partition tokens</a>\r\n==============================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n```scala\r\n\r\nimport scala.concurrent.Await\r\nimport scala.concurrent.duration._\r\nimport com.websudos.phantom.dsl._\r\n\r\nsealed class ExampleRecord2 extends CassandraTable[ExampleRecord2, ExampleModel] {\r\n\r\n  object id extends UUIDColumn(this) with PartitionKey[UUID]\r\n  object order_id extends LongColumn(this) with ClusteringOrder[Long] with Descending\r\n  object timestamp extends DateTimeColumn(this)\r\n  object name extends StringColumn(this)\r\n  object props extends MapColumn[ExampleRecord2, ExampleRecord, String, String](this)\r\n  object test extends OptionalIntColumn(this)\r\n\r\n  override def fromRow(row: Row): ExampleModel = {\r\n    ExampleModel(id(row), name(row), props(row), timestamp(row), test(row));\r\n  }\r\n}\r\n\r\n\r\nval orderedResult = Await.result(Articles.select.where(_.id gtToken one.get.id ).fetch, 5000 millis)\r\n\r\n```\r\n\r\n<a id=\"partition-token-operators\">PartitionToken operators</a>\r\n===============================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n| Operator name      | Description                                                              |\r\n| ------------------ | ------------------------------------------------------------                                             |\r\n| eqsToken           | The \"equals\" operator. Will match if the objects are equal                                               |\r\n| gtToken            | The \"greater than\" operator. Will match a the record is greater than the argument                        |\r\n| gteToken           | The \"greater than or equals\" operator. Will match a the record is greater than the argument              |\r\n| ltToken            | The \"lower than\" operator. Will match a the record that is less than the argument and exists             |\r\n| lteToken           | The \"lower than or equals\" operator. Will match a the record that is less than the argument              |\r\n\r\nFor more details on how to use Cassandra partition tokens, see [SkipRecordsByToken.scala]( https://github.com/websudos/phantom/blob/develop/phantom-test/src/test/scala/com/websudos/phantom/dsl/SkipRecordsByToken.scala)\r\n\r\n\r\n<a id=\"time-series\">Cassandra Time Series</a>\r\n=============================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nphantom supports Cassandra Time Series. To use them, simply mixin ```com.websudos.phantom.keys.ClusteringOrder``` and either ```Ascending``` or ```Descending```.\r\n\r\nRestrictions are enforced at compile time.\r\n\r\n```scala\r\n\r\nimport com.websudos.phantom.dsl._\r\n\r\nsealed class ExampleRecord3 extends CassandraTable[ExampleRecord3, ExampleModel] with LongOrderKey[ExampleRecod3, ExampleRecord] {\r\n\r\n  object id extends UUIDColumn(this) with PartitionKey[UUID]\r\n  object timestamp extends DateTimeColumn(this) with ClusteringOrder[DateTime] with Ascending\r\n  object name extends StringColumn(this)\r\n  object props extends MapColumn[ExampleRecord2, ExampleRecord, String, String](this)\r\n  object test extends OptionalIntColumn(this)\r\n\r\n  override def fromRow(row: Row): ExampleModel = {\r\n    ExampleModel(id(row), name(row), props(row), timestamp(row), test(row));\r\n  }\r\n}\r\n```\r\n\r\nAutomatic schema generation can do all the setup for you.\r\n\r\n\r\n<a id=\"compound-keys\">Compound keys</a>\r\n=======================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nPhantom also supports using Compound keys out of the box. The schema can once again by auto-generated.\r\n\r\nA table can have only one ```PartitionKey``` but several ```PrimaryKey``` definitions. Phantom will use these keys to build a compound value. Example scenario, with the compound key: ```(id, timestamp, name)```\r\n\r\n```scala\r\n\r\nimport com.websudos.phantom.dsl._\r\n\r\nsealed class ExampleRecord3 extends CassandraTable[ExampleRecord3, ExampleModel] {\r\n\r\n  object id extends UUIDColumn(this) with PartitionKey[UUID]\r\n  object order_id extends LongColumn(this) with ClusteringOrder[Long] with Descending\r\n  object timestamp extends DateTimeColumn(this) with PrimaryKey[DateTime]\r\n  object name extends StringColumn(this) with PrimaryKey[String]\r\n  object props extends MapColumn[ExampleRecord2, ExampleRecord, String, String](this)\r\n  object test extends OptionalIntColumn(this)\r\n\r\n  override def fromRow(row: Row): ExampleModel = {\r\n    ExampleModel(id(row), name(row), props(row), timestamp(row), test(row));\r\n  }\r\n}\r\n```\r\n\r\n<a id=\"secondary-keys\">CQL 3 Secondary Keys</a>\r\n===============================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nWhen you want to use a column in a ```where``` clause, you need an index on it. Cassandra data modeling is out of the scope of this writing, \r\nbut phantom offers ```com.websudos.phantom.keys.Index``` to enable querying.\r\n\r\nThe CQL 3 schema for secondary indexes can also be auto-generated with ```ExampleRecord4.create()```.\r\n\r\n```SELECT``` is the only query you can perform with an ```Index``` column. This is a Cassandra limitation. The relevant tests are found [here](https://github.com/websudos/phantom/blob/develop/phantom-test/src/test/scala/com/websudos/phantom/dsl/specialized/SecondaryIndexTest.scala).\r\n\r\n\r\n```scala\r\n\r\nimport com.websudos.phantom.dsl._\r\n\r\nsealed class ExampleRecord4 extends CassandraTable[ExampleRecord4, ExampleModel] {\r\n\r\n  object id extends UUIDColumn(this) with PartitionKey[UUID]\r\n  object order_id extends LongColumn(this) with ClusteringOrder[Long] with Descending\r\n  object timestamp extends DateTimeColumn(this) with Index[DateTime]\r\n  object name extends StringColumn(this) with Index[String]\r\n  object props extends MapColumn[ExampleRecord2, ExampleRecord, String, String](this)\r\n  object test extends OptionalIntColumn(this)\r\n\r\n  override def fromRow(row: Row): ExampleModel = {\r\n    ExampleModel(id(row), name(row), props(row), timestamp(row), test(row));\r\n  }\r\n}\r\n```\r\n\r\n<a id=\"async-iterators\">Asynchronous iterators for large record sets</a>\r\n========================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nPhantom comes packed with CQL rows asynchronous lazy iterators to help you deal with billions of records.\r\nphantom iterators are based on Play iterators with very lightweight integration.\r\n\r\nThe functionality is identical with respect to asynchronous, lazy behaviour and available methods.\r\nFor more on this, see this [Play tutorial](\r\nhttp://mandubian.com/2012/08/27/understanding-play2-iteratees-for-normal-humans/)\r\n\r\n\r\nUsage is trivial. If you want to use ```slice, take or drop``` with iterators, the partitioner needs to be ordered.\r\n\r\n```scala\r\n\r\nimport scala.concurrent.Await\r\nimport scala.concurrent.duration._\r\nimport com.websudos.phantom.dsl._\r\n\r\n\r\nsealed class ExampleRecord3 extends CassandraTable[ExampleRecord3, ExampleModel] {\r\n\r\n  object id extends UUIDColumn(this) with PartitionKey[UUID]\r\n  object order_id extends LongColumn(this) with ClusteringOrder[Long] with Descending\r\n  object timestamp extends DateTimeColumn(this) with PrimaryKey[DateTime]\r\n  object name extends StringColumn(this) with PrimaryKey[String]\r\n  object props extends MapColumn[ExampleRecord2, ExampleRecord, String, String](this)\r\n  object test extends OptionalIntColumn(this)\r\n\r\n  override def fromRow(row: Row): ExampleModel = {\r\n    ExampleModel(id(row), name(row), props(row), timestamp(row), test(row));\r\n  }\r\n}\r\n\r\nobject ExampleRecord3 extends ExampleRecord3 {\r\n  def getRecords(start: Int, limit: Int): Future[Set[ExampleModel]] = {\r\n    select.fetchEnumerator.slice(start, limit).collect\r\n  }\r\n}\r\n\r\n```\r\n\r\n<a id=\"batch-statements\">Batch statements</a>\r\n=============================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nphantom also brrings in support for batch statements. To use them, see [IterateeBigTest.scala](https://github.com/websudos/phantom/blob/develop/phantom-dsl/src/test/scala/com/websudos/phantom/iteratee/IterateeBigTest.scala)\r\n\r\nWe have tested with 10,000 statements per batch, and 1000 batches processed simultaneously. Before you run the test, beware that it takes ~40 minutes.\r\n\r\nBatches use lazy iterators and daisy chain them to offer thread safe behaviour. They are not memory intensive and you can expect consistent processing speed even with 1 000 000 statements per batch.\r\n\r\nBatches are immutable and adding a new record will result in a new Batch, just like most things Scala, so be careful to chain the calls.\r\n\r\nphantom also supports COUNTER batch updates and UNLOGGED batch updates.\r\n\r\n\r\n<a id=\"logged-batch-statements\">LOGGED batch statements</a>\r\n===========================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n```scala\r\n\r\nimport com.websudos.phantom.dsl._\r\n\r\nBatch.logged\r\n    .add(ExampleRecord.update.where(_.id eqs someId).modify(_.name setTo \"blabla\"))\r\n    .add(ExampleRecord.update.where(_.id eqs someOtherId).modify(_.name setTo \"blabla2\"))\r\n    .future()\r\n\r\n```\r\n\r\n<a id=\"counter-batch-statements\">COUNTER batch statements</a>\r\n============================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n```scala\r\n\r\nimport com.websudos.phantom.dsl._\r\n\r\nBatch.counter\r\n    .add(ExampleRecord.update.where(_.id eqs someId).modify(_.someCounter increment 500L))\r\n    .add(ExampleRecord.update.where(_.id eqs someOtherId).modify(_.someCounter decrement 300L))\r\n    .future()\r\n```\r\n\r\n<a id=\"unlogged-batch-statements\">UNLOGGED batch statements</a>\r\n============================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\n```scala\r\n\r\nimport com.websudos.phantom.dsl._\r\n\r\nBatch.unlogged\r\n    .add(ExampleRecord.update.where(_.id eqs someId).modify(_.name setTo \"blabla\"))\r\n    .add(ExampleRecord.update.where(_.id eqs someOtherId).modify(_.name setTo \"blabla2\"))\r\n    .future()\r\n\r\n```\r\n\r\n<a id=\"thrift-integration\">Thrift integration</a>\r\n=================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nWe use Apache Thrift extensively for our backend services. phantom is very easy to integrate with Thrift models and uses ```Twitter Scrooge``` to compile them. \r\nThrift integration is optional and available via ```\"com.websudos\" %% \"phantom-thrift\"  % phantomVersion```.\r\n\r\n```thrift\r\nnamespace java com.websudos.phantom.sample.ExampleModel\r\n\r\nstuct ExampleModel {\r\n  1: required i32 id,\r\n  2: required string name,\r\n  3: required Map&lt;string, string&gt; props,\r\n  4: required i32 timestamp\r\n  5: optional i32 test\r\n}\r\n```\r\n\r\n<a id=\"apache-zookeeper-integration\">Apache ZooKeeper Integration</a>\r\n==================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nIf you have never heard of Apache ZooKeeper before, a much better place to start is [here](http://zookeeper.apache.org/). Phantom offers a complete set of features for ZooKeeper integration using the [finagle-zookeeper](https://github.com/p-antoine/finagle-zookeeper) project.\r\n\r\n\r\n<a id=\"zookeeper-connectors\">ZooKeeper Connectors</a>\r\n===========================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nUsing a set of conventions phantom can automate the entire process of using ZooKeeper in a distributed environment. Phantom will deal with a large series of concerns for you, specifically:\r\n\r\n- Creating a ZooKeeper client and initialising it in due time.\r\n- Fetching and parsing a sequence of Cassandra ports from ZooKeeper.\r\n- Creating a Cluster configuration based on the sequence of Cassandra ports available in ZooKeeper.\r\n- Creating an implicit session for queries to execute.\r\n\r\nThe entire process described above is entirely automated with a series of sensible defaults available. More details on default implementations are available below. Bottom line, if you want to go custom, you may override at will, if you just want to get something working as fast as possible, then ```phantom-zookeeper``` can do everything for you.\r\n\r\n<a id=\"the-simple-cassandra-connector\">The simple Cassandra Connector</a>\r\n==========================================================================================================\r\n\r\nThis implementation is a very simple way to connect to a running Cassandra node. This is not using ZooKeeper and it's not really indented for multi-node \r\ntesting or connections, but sometimes you just want to get things working immediately.\r\n\r\nThe implementation details are available [here](https://github\r\n.com/websudos/phantom/blob/develop/phantom-connectors/src/main/scala/com/websudos/phantom/connectors/SimpleCassandraConnector.scala),\r\nbut without further ado, this connector will attempt to connector to a local Cassandra, either embedded or not.\r\n\r\nInside Websudos, our port convention is ```9042``` for local Cassandra and ```9142``` for embedded. This is reflected in our ```cassandra.yaml``` \r\nconfiguration files. Overidding this is quite simple, although you will need to create your own pair of manager and connector.\r\n\r\n\r\n<a id=\"the-default-zookeeper-connector-and-default-zookeeper-mananager\">The DefaultZooKeeperConnector and DefaultZooKeeperManager</a>\r\n==========================================================================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nThe default implementation expects Cassandra IPs to be listed in a Sequence of ```host:port``` combinations, with ```:``` as a separator literal. It also expects the default path in ZooKeeper for Cassandra ports to be ```/cassandra``` and the sequence of ports should look like this:\r\n\r\n```host1:port1, host2:port2, host3:port3, host4:port4```\r\n\r\nPhantom will fetch the data found on the  ```/cassandra``` path on the ZooKeeper master and attempt to parse all ```host:port``` pairs to a ```Seq[InetSocketAddress]``` and build a ```com.datastax.driver.core.Cluster``` using the sequence of addresses.\r\n\r\nUsing that ```Cluster``` phantom will spawn an ```implicit session: com.datastax.driver.core.Session```. This session is the execution context of all queries inside a table definition. The ```DefaultZooKeeperManager```, found [here](https://github.com/websudos/phantom/blob/develop/phantom-zookeeper/src/main/scala/com/websudos/phantom/zookeeper/ZookeeperManager.scala), will do all the plumbing work for you. More details on the internals are available [here](https://github.com/websudos/phantom/blob/develop/phantom-zookeeper/src/main/scala/com/websudos/phantom/zookeeper/ZookeeperManager.scala#L51).\r\n\r\n<a id=\"testing-utilities\">phantom-testkit</a>\r\n==================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nNaturally, no job is considered truly done with the full power testing automation provided out-of-the box. This is exactly what we tried to achieve with the \r\ntesting utilities, giving you a very simple, easily extensible, yet highly sensible defaults. We wanted something that works for most things most of the time\r\nwith 0 integration work on your behalf, yet allowing you to go crazy and custom as you please if the scenario warrants it. \r\n\r\nWith that design philosophy in mind, we've created two kinds of tests, 1 running with a ```SimpleCassandraConnector```, \r\nwith the implementation found [here](https://github.com/websudos/phantom/blob/develop/phantom-testkit/src/main/scala/com/websudos/phantom/testkit\r\n/SimpleCassandraConnector.scala), where the testing utilities will auto-spawn an Embedded Cassandra database with the right version and the right settings,\r\nrun all the tests and cleanup after tests are done.\r\n\r\nThe other, more complex implementation, targets users who want to use phantom/Cassandra in a distributed environment. This is an easy way to automate \r\nmulti-DC or multi-cluster tests via service discovery with Apache ZooKeeper. More details are available right above. The ```BaseTest``` implementation, \r\nwhich uses a ```DefaultZooKeeperConnector```, is found [here](https://github.com/websudos/phantom/blob/develop/phantom-testkit/src/main/scala/com/websudos/phantom/testkit/BaseTest.scala), and it follows the pattern described above.\r\n\r\n\r\nThere are 4 core implementations available:\r\n\r\n\r\n| Name    | Description                                                                         | ZooKeeper support | Auto-embedding support |\r\n| ------- | --------------------------------------------------------------------------------------------------------- | ----------- | ---------------------- |\r\n| CassandraFlatSpec             | Simple FlatSpec trait mixin, based on ```org.scalatest.FlatSpec```                  | No          | Yes                    |\r\n| CassandraFeatureSpec          | Simple FeatureSpec trait mixin, based on ```org.scalatest.FeatureSpec```            | No          | Yes                    |\r\n| BaseTest                      | ZooKeeper powered FlatSpec trait mixin, based on ```org.scalatest.FlatSpec```       | Yes         | Yes                    |\r\n| FeatureBestTest               | ZooKeeper powered FeatureSpec trait mixin, based on ```org.scalatest.FeatureSpec``` | Yes         | Yes                    |\r\n\r\n \r\n \r\nUsing the built in testing utilities is very simple. In most cases, you use one of the first two base implementations, \r\neither ```CassandraFlatSpec``` or ```CassandraFeatureSpec```, based on what kind of tests you like writing(flat or feature).\r\n\r\n\r\nTo get started with phantom tests, the usual steps are as follows:\r\n\r\n- Create a global method to initialise all your tables using phantom's auto-generation capability.\r\n- Create a global method to cleanup and truncate your tables after tests finish executing.\r\n- Create a root specification file that you plan to use for all your tests.\r\n\r\n\r\n```scala\r\n\r\nimport scala.concurrent.{ Await, Future }\r\nimport scala.concurrent.duration._\r\nimport com.websudos.phantom.dsl._\r\n\r\n\r\nobject DatabaseService {\r\n  def init(): Future[List[ResultSet]] = {\r\n    val create = Future.sequence(List(\r\n      Table1.create.future(),\r\n      Table2.create.future()\r\n    ))\r\n\r\n    Await.ready(create, 5.seconds)\r\n  }\r\n\r\n   def cleanup(): Future[List[ResultSet]] = {\r\n    val truncate = Future.sequence(List(\r\n      Table1.truncate.future(),\r\n      Table2.truncate.future()\r\n    ))\r\n    Await.ready(truncate, 5.seconds)\r\n  }\r\n}\r\n\r\nimport com.websudos.phantom.testkit._\r\n\r\ntrait CustomSpec extends CassandraFlatSpec {\r\n\r\n   override def beforeAll(): Unit = {\r\n     super.beforeAll()\r\n     DatabaseService.init()\r\n   }\r\n\r\n   override def afterAll(): Unit = {\r\n     super.afterAll()\r\n     DatabaseService.cleanup()\r\n   }\r\n}\r\n```\r\n\r\nRunning your database tests with phantom is now trivial. A great idea is to use asynchronous testing patterns and future sequencers to get the best possible\r\nperformance even out of your tests. Now all your other test suites that need a running database would look like this:\r\n\r\n```scala\r\n\r\nimport com.websudos.phantom.dsl._\r\nimport com.websudos.util.testing._\r\n\r\nclass UserDatabaseServiceTest extends CustomSpec {\r\n  it should \"register a user from a model\" in {\r\n    val user = //.. create a user\r\n\r\n    // A for-yield will get de-sugared to a flatMap chain, but in effect you get a sequence that says:\r\n    // First write, then fetch by id. The beauty of it is the first future will only complete when the user has been written\r\n    // So you have an async sequence guarantee that the \"getById\" will be done only after the user is actually available.\r\n    val chain = for {\r\n      store <- UserDatabaseService.register(user)\r\n      get <- UserDatabaseService.getById(user.id)\r\n    } yield get\r\n\r\n    // The \"successful\" method comes from com.websudos.util.testing._ in our util project.\r\n    chain.successful {\r\n      result => {\r\n\r\n        // result is now Option[User]\r\n\r\n        result.isDefined shouldEqual true\r\n        result.get shouldEqual user\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\n```\r\n\r\n\r\n\r\nIf you are using ZooKeeper and you want to run tests through a full ZooKeeper powered cycle, where Cassandra settings are retrieved from a ZooKeeper that \r\ncan either be running locally or auto-spawned if none is found, pick one of the last two base suites.\r\n \r\n \r\n\r\n<a id=\"auto-embedded-cassandra\">Auto-embedded Cassandra</a>\r\n===========================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nPhantom spares you of the trouble to spawn your own Cassandra server during tests. The implementation of this is based on the [cassandra-unit]\r\n(https://github.com/jsevellec/cassandra-unit) project. Phantom will automatically pick the right version of Cassandra, \r\nhowever do be careful. We often tend to use the latest version as we do our best to keep up with the latest features.\r\n\r\nYou may use a brand new phantom feature, see the tests passing with flying colours locally and then get bad errors in production. The version of Cassandra \r\ncovered by the latest phantom release and used for embedding is written at the very top of this readme.\r\n\r\n<a id=\"running-the-tests-locally\">Running the tests locally</a>\r\n==================================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nphantom uses the ```phantom-testkit``` module to run tests without a local Cassandra server running.\r\nThere are no pre-requisites for running the tests. Phantom will automatically load an Embedded Cassandra with the right version, \r\nrun all the tests and do the cleanup afterwards. Read more on the testing utilities to see how you can achieve the same thing in your own database tests.\r\n\r\nIf a local Cassandra installation is found running on ```localhost:9042```, phantom will attempt to use that instead. Some of the version based logic\r\nis found directly inside phantom, although advanced compatibility and protocol version detection has been a task we left to our dear partners at Datastax\r\nas we've felt re-implementing that concern in Scala would bring no significant value add.\r\n\r\nPhantom uses multiple SBT configurations to distinguish between two kinds of tests, normal and performance tests. Performance tests are not run\r\nduring Travis CI runs and we usually run them manually when serious changes are made to the underlying Twitter Spool and Play Iterator based iterators, events that are very rare indeed.\r\n\r\n- Use ```sbt test``` to run the normal test suite which should finish pretty quickly, within 2 minutes. \r\n- Use ```sbt perf:test``` if you have a lot of time on your hands and you are debugging performance issues with the framework. This will take 40 -50 minutes.\r\n\r\n\r\n<a id=\"contributors\">Contributors</a>\r\n=====================================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nPhantom was developed at websudos as an in-house project. All Cassandra integration at Websudos goes through phantom, and nowadays it's safe to say most\r\nScala/Cassandra users in the world rely on phantom.\r\n\r\n* Flavian Alexandru ([@alexflav23](https://github.com/alexflav23)) - maintainer\r\n* Viktor Taranenko ([@viktortnk](https://github.com/viktortnk))\r\n* Bartosz Jankiewicz ([@bjankie1](https://github.com/bjankie1)\r\n* Eugene Zhulenev ([@ezhulenev](https://github.com/ezhulenev)\r\n* Benjamin Edwards ([@benjumanji](https://github.com/benjumanji)\r\n* Stephen Samuel ([@sksamuel](https://github.com/sksamuel)\r\n* Tomasz Perek ([@tperek](https://github.com/tperek)\r\n* Evan Chan ([@evanfchan](https://github.com/evanfchan)\r\n\r\n<a id=\"copyright\">Copyright</a>\r\n===============================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nSpecial thanks to Viktor Taranenko from WhiskLabs, who gave us the original idea.\r\n\r\nCopyright 2013 - 2015 websudos.\r\n\r\n\r\nContributing to phantom\r\n=======================\r\n<a href=\"#table-of-contents\">back to top</a>\r\n\r\nContributions are most welcome! Use GitHub for issues and pull requests and we will happily help out in any way we can!\r\n\r\n<a id=\"git-flow\">Using GitFlow</a>\r\n==================================\r\n\r\nTo contribute, simply submit a \"Pull request\" via GitHub.\r\n\r\nWe use GitFlow as a branching model and SemVer for versioning.\r\n\r\n- When you submit a \"Pull request\" we require all changes to be squashed.\r\n- We never merge more than one commit at a time. All the n commits on your feature branch must be squashed.\r\n- We won't look at the pull request until Travis CI says the tests pass, make sure tests go well.\r\n\r\n<a id=\"style-guidelines\">Scala Style Guidelines</a>\r\n===================================================\r\n\r\nIn spirit, we follow the [Twitter Scala Style Guidelines](http://twitter.github.io/effectivescala/).\r\nWe will reject your pull request if it doesn't meet code standards, but we'll happily give you a hand to get it right.\r\n\r\nSome of the things that will make us seriously frown:\r\n\r\n- Blocking when you don't have to. It just makes our eyes hurt when we see useless blocking.\r\n- Testing should be thread safe and fully async, use ```ParallelTestExecution``` if you want to show off.\r\n- Writing tests should use the pre-existing tools, they bring in EmbeddedCassandra, Zookeeper and other niceties, allowing us to run multi-datacenter tests.\r\n- Use the common patterns you already see here, we've done a lot of work to make it easy.\r\n- Don't randomly import stuff. We are very big on alphabetized clean imports.\r\n- Tests must pass on both the Oracle and OpenJDK JVM implementations. The only sensitive bit is the Scala reflection mechanism used to detect columns.\r\n\r\nYourKit Java Profiler\r\n==================\r\n\r\n![yourkit](https://s3-eu-west-1.amazonaws.com/websudos/oss/yklogo.png \"YourKit Java Profiler\")\r\n\r\nWe are very grateful to have the open source license support of YourKit, the most advanced Java profiler.\r\n\r\nYourKit is the very core of our performance bottleneck testing, and without it phantom would still be a painfully slow tool.\r\n\r\n[YourKit Java profiler](https://www.yourkit.com/java/profiler/index.jsp)\r\n","google":"UA-51902884-4","note":"Don't delete this file! It's used internally to help with page regeneration."}